{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshayAI007/Cardiovascular-disease-risk-prediction-using-Machine-learning/blob/main/Cardiovascular_Risk_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Cardiovascular Risk Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Akshay Bawaliwale\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiovascular disease is a leading cause of death worldwide, and early prediction of cardiovascular risk can help in timely intervention and prevention of the disease. Machine learning techniques have shown promising results in predicting cardiovascular risk by analyzing various risk factors.\n",
        "\n",
        "The goal of this project is to develop a machine learning model to predict the 10-year risk of cardiovascular disease in individuals using a dataset of demographic, clinical, and laboratory data.\n",
        "\n",
        "The dataset used in this project is the Framingham Heart Study dataset, which is a widely used dataset for cardiovascular risk prediction. It contains data on 3,390 participants, who were followed up for ten years to track cardiovascular events. The dataset includes 17 variables such as age, sex, blood pressure, cholesterol levels, smoking status, and diabetes status.\n",
        "\n",
        "The first step in this project is to perform data preprocessing, which includes handling missing values, encoding categorical variables, and scaling numerical variables. After preprocessing, the dataset is split into training and testing sets using a 70:20 ratio.\n",
        "\n",
        "Various machine learning algorithms are applied to the training data, including logistic regression, KNN, XGBoost, SVC, and random forest, Naive Bayes Classifier. These algorithms are chosen as they have been shown to perform well in cardiovascular risk prediction. The algorithms are trained on the training data, and their performance is evaluated using the testing data.\n",
        "\n",
        "The evaluation metrics used in this project include accuracy, precision, recall, and area under the receiver operating characteristic curve (AUC-ROC). These metrics help in assessing the performance of the machine learning model.\n",
        "\n",
        "The results show that the XGBoost performs the best, with an accuracy of 0.89, precision of 0.92, recall of 0.85, and AUC-ROC of 0.89. This indicates that the model has a good overall performance in predicting cardiovascular risk.\n",
        "\n",
        "Further analysis is performed to identify the most important features in the dataset. The feature importance plot shows that age, education, prevalentHyp,and cigarettes per day are the top important features in predicting cardiovascular risk. This information can help in identifying high-risk individuals and implementing preventive measures.\n",
        "\n",
        "In conclusion, this project demonstrates the effectiveness of machine learning techniques in predicting cardiovascular risk using the Framingham Heart Study dataset. The developed machine learning model can be used by healthcare professionals to identify individuals at high risk of cardiovascular disease and take preventive measures to reduce the risk."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AkshayAI007/Cardiovascular-disease-risk-prediction-using-Machine-learning.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cardiovascular disease is a major cause of morbidity and mortality worldwide. Early identification and management of individuals at high risk of developing cardiovascular disease is crucial for the prevention of the disease. Traditional risk prediction models, such as the Framingham Risk Score, have limitations in their accuracy and do not account for the complex interactions between various risk factors. Machine learning techniques have shown promising results in improving the accuracy of cardiovascular risk prediction by integrating various risk factors and identifying non-linear interactions. However, there is a need for developing and validating machine learning models that can accurately predict cardiovascular risk using demographic, clinical, and laboratory data. The goal of this project is to address this need by developing and evaluating a machine learning model for predicting the 10-year risk of cardiovascular disease using the Framingham Heart Study dataset.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compatible versions of modules\n",
        "!sudo apt-get install python3.9\n",
        "!pip install scikit-learn==1.1.2"
      ],
      "metadata": {
        "id": "oLg3khNLppTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "import seaborn as sns\n",
        "\n",
        "## Machine Learning\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "## Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\n",
        "\n",
        "## Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Dataset\n",
        "path='/content/drive/MyDrive/Projects/Cardiovascular_disease_risk_prediction/data_cardiovascular_risk.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Last 5 entries\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "-Pv5Yq08rdAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Dataset Size\")\n",
        "print(\"Rows = {} and  Columns = {}\".format(df.shape[0], df.shape[1]))"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar = False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets for predicting cardiovascular risk typically encompass a variety of risk factors that can influence an individual's likelihood of developing cardiovascular disease. These factors encompass aspects such as age, gender, blood pressure, cholesterol levels, smoking habits, and a history of cardiovascular disease. Additionally, these datasets may encompass variables like body mass index and diabetes. It's worth noting that these datasets often exhibit some missing values, with glucose and education variables being particularly notable in this regard."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demographic:**\n",
        "\n",
        "1) Age: Age of the patient.\n",
        "\n",
        "2) Sex: male or female(\"M\" or \"F\")\n",
        "\n",
        "**Behavioral:**\n",
        "\n",
        "3) is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\").\n",
        "\n",
        "4) CigsPerDay: the number of cigarettes that the person smoked on average in one day.(countinous type feature because a person can smoke 'n' times a day)\n",
        "\n",
        "**Medical(history):**\n",
        "\n",
        "5) BPMeds: whether or not the patient was on blood pressure medication.\n",
        "\n",
        "6) Prevalent Stroke: whether or not the patient had previously had a stroke.\n",
        "\n",
        "7) Prevalent Hyp: whether or not the patient was hypertensive.\n",
        "\n",
        "8) Diabetes: whether or not the patient had diabetes.\n",
        "\n",
        "**Medical(current):**\n",
        "\n",
        "9) Tot Chol: total cholesterol level.\n",
        "\n",
        "10) Sys BP: systolic blood pressure.\n",
        "\n",
        "11) Dia BP: diastolic blood pressure.\n",
        "\n",
        "12) BMI: Body Mass Index.\n",
        "\n",
        "13) Heart Rate: heart rate.\n",
        "\n",
        "14) Glucose: glucose level.\n",
        "\n",
        "**Target feature(class of risk):**\n",
        "\n",
        "15) TenYearCHD: 10-year risk of coronary heart disease CHD (“1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \", i , \"is\" , df[i].nunique(), \".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Separating the categorical and continous variable and storing them\n",
        "categorical_variable=[]\n",
        "continous_variable=[]\n",
        "\n",
        "for i in df.columns:\n",
        "  if i == 'id':\n",
        "    pass\n",
        "  elif df[i].nunique() <5:\n",
        "    categorical_variable.append(i)\n",
        "  elif df[i].nunique() >= 5:\n",
        "    continous_variable.append(i)\n",
        "\n",
        "print(categorical_variable)\n",
        "print(continous_variable)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summing null values\n",
        "print('Missing Data Count')\n",
        "df.isna().sum()[df.isna().sum() > 0].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "6T7VwzrPsq9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Missing Data Percentage')\n",
        "print(round(df.isna().sum()[df.isna().sum() > 0].sort_values(ascending=False)/len(df)*100,2))"
      ],
      "metadata": {
        "id": "KRsXc-J5ste-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the column that contains null values\n",
        "null_column_list= ['glucose','education','BPMeds','totChol','cigsPerDay','BMI','heartRate']\n",
        "# plotting box plot\n",
        "plt.figure(figsize=(15,8))\n",
        "df[null_column_list].boxplot()"
      ],
      "metadata": {
        "id": "DYNj0cEXsvbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of colors\n",
        "colors = sns.color_palette(\"husl\", len(null_column_list))\n",
        "\n",
        "# Create a figure with 8 subplots (2 rows, 4 columns)\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
        "\n",
        "# Flatten the axes array to make it easier to iterate over\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate over the null column list and plot each column's distribution\n",
        "for i, column in enumerate(null_column_list):\n",
        "    # Select the current axis\n",
        "    ax = axes[i]\n",
        "    # Plot a distplot of the current column with a different color\n",
        "    sns.distplot(df[column], ax=ax, color=colors[i])\n",
        "    # Add a title to the plot\n",
        "    ax.set_title(column)\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(len(null_column_list), len(axes)):\n",
        "    axes[j].remove()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jfzKaEwGsxvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a well-known fact that the appropriate measure of central tendency depends on the nature of the data. Typically, the mean is used for data that follows a normal distribution and does not contain any outliers. On the other hand, when dealing with numerical, continuous data that contains extreme values or outliers, the median is the preferred measure of central tendency. For categorical data, the mode is used.\n",
        "\n",
        "Based on the outliers and distribution of the data, we have determined that the following measures of central tendency are appropriate for imputing the null values in the following columns:\n",
        "\n",
        "**\"education\" , \"BPMeds\"** -> mode: As \"education\" and \"BPMeds\" is a categorical variable, the mode is the most appropriate measure of central tendency. The mode represents the most frequently occurring value in the distribution and can provide insight into the most common level of education in the dataset.\n",
        "\n",
        "**\"glucose\",\"totChol\", \"cigsPerDay\", \"BMI\", \"heartRate\"** -> median: Since this are numerical, continuous variable that contain extreme values or outliers, we have chosen the median as the appropriate measure of central tendency. The median is less sensitive to extreme values than the mean and provides a representative value for the central tendency of the distribution."
      ],
      "metadata": {
        "id": "o_Wu3Uzfs1Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing missing values with median or mode\n",
        "df.fillna({'glucose': df['glucose'].median(),\n",
        "           'education': df['education'].mode()[0],\n",
        "           'BPMeds': df['BPMeds'].mode()[0],\n",
        "           'totChol': df['totChol'].median(),\n",
        "           'cigsPerDay': df['cigsPerDay'].median(),\n",
        "           'BMI': df['BMI'].median(),\n",
        "           'heartRate': df['heartRate'].median()}, inplace=True)"
      ],
      "metadata": {
        "id": "k1IwZRdHs15b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We addressed the issue of missing data by employing a combined approach of imputation using median and mode values. Specifically, for the glucose and totChol columns, as well as cigsPerDay, BMI, and heartRate, we substituted the missing values with the median of the available non-missing values. Conversely, for the education and BPMeds columns, we filled in missing values with the mode, which represents the most frequently occurring value among the non-missing data points.\n",
        "\n",
        "These methods of imputation, utilizing median and mode values, are widely recognized and commonly employed to handle missing data. Median imputation is a preferred choice for continuous variables due to its robustness against outliers when compared to mean imputation. On the other hand, mode imputation is commonly used for categorical variables or discrete variables with a limited number of possible values.\""
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 1**\n",
        " **Which age group is more susceptible to developing coronary heart disease?**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Set the figure size\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "# Create a boxplot to compare the age distribution of patients by sex and CHD risk level\n",
        "sns.boxplot(x=\"sex\", y=\"age\", hue=\"TenYearCHD\", data= df, ax=ax)\n",
        "# Set the title and labels\n",
        "ax.set_title(\"Age Distribution of Patients by Sex and CHD Risk Level\")\n",
        "ax.set_xlabel(\"Sex\")\n",
        "ax.set_ylabel(\"Age\")\n",
        "# Adding a legend with appropriate labels\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, [\"No Risk\", \"At Risk\"], loc=\"best\")\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is a boxplot that visualizes the age distribution of patients by sex and CHD (coronary heart disease) risk level. It was likely chosen to gain insights into how age, sex, and CHD risk level may be related in this dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a noticeable difference in the age distribution of patients who are at risk for CHD compared to those who are not at risk. Patients at risk for CHD tend to be older than those who are not at risk, regardless of sex."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information derived from this chart could prove valuable for businesses operating within the healthcare industry. For instance, companies specializing in the manufacture of medical equipment or medications for coronary heart disease (CHD) might contemplate tailoring their products towards older patients or individuals with a heightened risk of CHD. Nonetheless, it is crucial to emphasize that this chart in isolation may lack the depth required for making informed business decisions. A more comprehensive analysis is necessary to gain a thorough understanding of the interplay between age, gender, CHD risk, and other pertinent variables.\n",
        "It's important to highlight that there are no indications of adverse growth trends evident in this chart."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 2**\n",
        "**Does gender affect the risk of coronary heart disease in the dataset?**####"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.countplot(x='sex', hue='TenYearCHD', data= df)\n",
        "plt.title('Frequency of CHD cases by gender')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is a countplot that visualizes the frequency of CHD (coronary heart disease) cases by gender in the dataset. It was likely chosen to investigate whether gender affects the risk of CHD in the dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that there are more cases of CHD among men than women in the dataset. However, this difference is not drastic, as the number of cases of CHD is relatively similar between men and women. Additionally, the chart shows that there are more cases of no risk for CHD among women compared to men."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The findings derived from this chart hold potential value for healthcare service and product providers. For instance, businesses involved in the manufacturing of medical devices or medications for coronary heart disease (CHD) might find it advantageous to target both genders. However, a more substantial emphasis on men may be warranted, given their seemingly higher risk for CHD as indicated by this dataset."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 3**\n",
        "\n",
        "**Do smokers have a higher risk of developing coronary heart disease?**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.countplot(x='is_smoking', hue='TenYearCHD', data= df)\n",
        "plt.title('A Comparison of Smokers and Non-Smokers')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is a countplot that visualizes the frequency of CHD (coronary heart disease) cases among smokers and non-smokers. It was likely chosen to gain insights into how smoking may be related to the risk of CHD in this dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates that individuals who engage in smoking appear to exhibit a heightened risk of coronary heart disease (CHD) compared to their non-smoking counterparts within this dataset. Precisely, a greater percentage of smoking individuals are identified as being at risk for CHD when contrasted with those who abstain from smoking. These observations indicate that smoking may play a role in influencing the CHD risk within this dataset."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart does not reveal any indications of adverse growth trends. Its focus is solely on depicting the incidence of coronary heart disease (CHD) cases among both smokers and non-smokers, omitting insights into other potentially pertinent factors like age or various lifestyle variables. Furthermore, it's worth noting that the dataset's representativeness may be limited, which could curtail the applicability of the insights gleaned from this chart to the broader population."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 4**\n",
        "**How much smoking affect coronary heart disease?**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.countplot(x= df['cigsPerDay'],hue= df['TenYearCHD'])\n",
        "plt.title('How much smoking affect CHD?')\n",
        "plt.legend(['No Risk','At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This countplot visually represents the correlation between the daily cigarette consumption and the risk of coronary heart disease (CHD) within this dataset. The selection of this chart type is likely aimed at obtaining a better understanding of the potential link between the intensity of smoking and the likelihood of CHD risk"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates that individuals who either smoke a greater number of cigarettes per day or do not smoke at all seem to face a heightened risk of coronary heart disease (CHD) in comparison to those who smoke fewer cigarettes daily. Concretely, a larger percentage of individuals who smoke 20 or more cigarettes per day are identified as being at risk for CHD when contrasted with those who smoke fewer cigarettes per day. These observations imply that the intensity of smoking may have a role in influencing the risk of CHD within this dataset"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses involved in the production of smoking cessation products or medications for coronary heart disease (CHD) may find it advantageous to contemplate a focus on heavy smokers, given their apparent elevated risk for CHD within this dataset."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 5**\n",
        "**Do patients taking medication for blood pressure have a higher risk of developing coronary heart disease?**\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Compute the cross-tabulation of BP medication and CHD risk\n",
        "ct = pd.crosstab(df['BPMeds'], df['TenYearCHD'], normalize='index')\n",
        "# Plot a stacked bar chart\n",
        "ct.plot(kind='bar', stacked=True, figsize=(8, 8))\n",
        "plt.title('Relationship between BP Medication and CHD Risk')\n",
        "plt.xlabel('BP Medication')\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylabel('Proportion')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stacked bar chart visually represents the correlation between patients' use of blood pressure medication and their susceptibility to coronary heart disease (CHD). It is probable that this chart was selected to explore the potential association between the usage of blood pressure medication and CHD risk within this dataset."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that individuals who are prescribed blood pressure medication appear to exhibit an elevated risk of coronary heart disease (CHD) when compared to those who do not receive such medication. More precisely, there is a noticeable disparity in the proportion of individuals at risk for CHD between those who are on blood pressure medication and those who are not. These observations imply that the usage of blood pressure medication may play a substantial role in influencing the CHD risk within this dataset."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firms specializing in the development of blood pressure (BP) medication or other remedies for hypertension may find it beneficial to focus on individuals with elevated blood pressure levels who also face a risk of coronary heart disease (CHD), irrespective of their current use of BP medication. This strategy could aid in the identification of patients who could benefit from more intensive treatment to mitigate their CHD risk."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6\n",
        "**Is a person who has had a stroke more susceptible to coronary heart disease?**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.countplot(x=df['prevalentStroke'], hue=df['TenYearCHD'])\n",
        "plt.title('Are people who had a stroke earlier more prone to CHD?')\n",
        "plt.legend(['No Risk', 'At Risk'], loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is a countplot illustrating a comparison of CHD risk levels among patients with a prior stroke history and those without such a history. The selection of this chart is likely motivated by an interest in exploring a potential link between experiencing a stroke and an increased susceptibility to CHD."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates an association between a prior history of stroke and an elevated risk of coronary heart disease (CHD) within this dataset. More precisely, the percentage of patients at risk for CHD is notably higher among those with a history of stroke compared to those without. These observations indicate a potential link between experiencing a stroke and an increased susceptibility to CHD within the dataset"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information gleaned from this chart holds potential significance for businesses operating within the realm of healthcare services or products associated with stroke or coronary heart disease (CHD). For instance, manufacturers of medications or treatments for stroke or CHD could contemplate directing their efforts towards patients who have experienced a stroke, recognizing them as a high-risk demographic for CHD.\n",
        "\n",
        "Furthermore, healthcare providers may consider implementing screening protocols for CHD risk among individuals who have a history of stroke. This could enable the delivery of targeted preventative measures or treatments to address potential risks and promote better patient outcomes."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7\n",
        "**Does having hypertension increase the risk of developing coronary heart disease?**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.countplot(x=df['prevalentHyp'], hue=df['TenYearCHD'])\n",
        "plt.title('Are hypertensive patients at more risk of CHD?')\n",
        "plt.legend(title='CHD Risk', labels=['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected this chart to visually represent the correlation between the presence of hypertension and the likelihood of developing coronary heart disease within the dataset."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates a correlation between prevalent hypertension and an increased likelihood of developing coronary heart disease (CHD) when compared to individuals without hypertension. More precisely, it indicates that the proportion of patients at risk for CHD is comparable among those with prevalent hypertension and those without this condition."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart demonstrates a connection between the presence of prevalent hypertension and an elevated probability of developing coronary heart disease (CHD) when contrasted with individuals lacking hypertension. Specifically, it highlights that the percentage of patients at risk for CHD is similar between those with prevalent hypertension and those without this condition"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 8**\n",
        "**Do individuals with diabetes have a higher risk of developing coronary heart disease?**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x=df['diabetes'], y=df['TenYearCHD'], hue=df['TenYearCHD'], estimator=lambda x: len(x) / len(df) * 100)\n",
        "plt.title('Proportion of patients with and without diabetes at CHD risk')\n",
        "plt.xlabel('Diabetes')\n",
        "plt.ylabel('Percentage')\n",
        "plt.legend(title='CHD Risk', labels=['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected this chart to represent the distribution of individuals in the dataset, categorizing them based on the presence or absence of diabetes, and examining their respective risks of developing coronary heart disease."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that individuals with diabetes have a higher likelihood of being susceptible to coronary heart disease in contrast to those who do not have diabetes."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the insights obtained can assist healthcare enterprises and practitioners in identifying patients with diabetes at an elevated risk level, necessitating additional evaluation, ongoing monitoring, and comprehensive management to mitigate the onset or advancement of coronary heart disease.\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 9**\n",
        "**Is there a correlation between total cholesterol levels and coronary heart disease?**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.boxplot(x='TenYearCHD', y='totChol', data=df)\n",
        "plt.title('Total Cholesterol Levels and CHD')\n",
        "plt.xlabel('TenYearCHD')\n",
        "plt.ylabel('Total Cholesterol Levels')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected the particular box plot as a means to address the query concerning the potential correlation between total cholesterol levels and the susceptibility to coronary heart disease development"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot reveals that individuals at risk of developing coronary heart disease tend to exhibit slightly elevated average total cholesterol levels compared to those not at risk. However, it's important to note that there is a degree of overlap in the cholesterol level ranges between these two groups"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights obtained can assist healthcare providers in assessing the influence of total cholesterol levels on the susceptibility to coronary heart disease (CHD) among their patients. The identification of individuals with elevated cholesterol levels enables the implementation of targeted interventions to mitigate their risk of CHD development. Such proactive measures can yield favorable effects on patient health outcomes and potentially result in cost reductions for healthcare providers over time"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 10**\n",
        "**What is the pairwise relationship between glucose levels, systolic blood pressure, diastolic blood pressure, and the risk of developing coronary heart disease?**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# select the columns of interest\n",
        "cols = ['glucose', 'sysBP', 'diaBP', 'TenYearCHD']\n",
        "\n",
        "# create the scatter plot matrix\n",
        "sns.pairplot(df[cols], hue='TenYearCHD', markers=['o', 's'])"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High glucose levels appear to be associated with an increased risk of developing coronary heart disease, as indicated by a higher concentration of orange (high-risk) points in the upper right quadrant of the glucose vs. TenYearCHD plot.\n",
        "\n",
        "High blood pressure levels (both systolic and diastolic) also appear to be associated with an increased risk of developing coronary heart disease, as indicated by a higher concentration of orange points in the upper right quadrants of the sysBP vs. TenYearCHD and diaBP vs. TenYearCHD plots.\n",
        "\n",
        "There may be some interaction effects between glucose and blood pressure on the risk of developing coronary heart disease, as indicated by the patterns of orange points in the glucose vs. sysBP and glucose vs. diaBP plots. However, further analysis is needed to explore these relationships in more detail."
      ],
      "metadata": {
        "id": "kRpRQROw0MDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was chosen to visualize the pairwise relationships between four variables: glucose levels, systolic blood pressure, diastolic blood pressure, and the risk of developing coronary heart disease. A pairplot was used to display all pairwise scatterplots, histograms along the diagonal."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot provides a visual representation of the interrelationships between glucose levels, systolic blood pressure, diastolic blood pressure, and the likelihood of coronary heart disease development. Along the diagonal, histograms display the distribution of each variable, while scatter plots illustrate the associations between pairs of variables. Notably, it's evident from the plot that individuals with elevated glucose levels tend to exhibit an increased risk of developing coronary heart disease. Likewise, individuals with elevated systolic and diastolic blood pressure levels appear to have an elevated risk of developing coronary heart disease"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights drawn from this chart do not point toward negative growth; rather, they have the potential to foster positive growth by aiding healthcare enterprises in the development of more efficacious prevention and treatment approaches. These strategies have the capacity to enhance patient outcomes and, in turn, may lead to potential reductions in healthcare expenditures"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 11**\n",
        "**Does cigarette smoking have a differential impact on the risk of developing coronary heart disease between males and females?**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# select the columns of interest\n",
        "cols = ['sex', 'cigsPerDay', 'TenYearCHD']\n",
        "\n",
        "# create a grouped scatter plot of TenYearCHD by cigsPerDay and sex\n",
        "sns.scatterplot(x='cigsPerDay', y='TenYearCHD', hue='sex', data=df)\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected this chart because it effectively visualizes the interplay between daily cigarette consumption, the probability of developing coronary heart disease, and how gender influences this dynamic. A scatter plot was employed to depict the data's distribution and to discern any underlying patterns or trends."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the chart, it is evident that individuals of both genders who smoke experience an elevated risk of developing coronary heart disease as the daily cigarette consumption rises. Nevertheless, the association between cigarette smoking and CHD risk exhibits greater prominence among males than females. Specifically, among males, those who consume more than 10 cigarettes per day demonstrate a notably heightened risk of CHD compared to their female counterparts.\""
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information gleaned from this chart offers valuable insights that can inform the efforts of public health organizations and businesses in designing tailored interventions aimed at reducing smoking prevalence and mitigating the onset of coronary heart disease (CHD). Particularly, there is an opportunity to focus on male smokers who exhibit an elevated risk. For instance, public health initiatives can be strategized to heighten awareness about the health hazards associated with smoking and offer assistance and resources to individuals seeking to quit. Similarly, businesses can consider implementing smoking cessation programs for their employees, promoting improved health and overall well-being."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Chart - 12**\n",
        "**Are there differences in the age and sex distributions between individuals with and without prevalent stroke?**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "sns.violinplot(x='prevalentStroke',y=\"age\",data=df, hue='sex', split='True', palette='rainbow')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is made clear that most of the prevalent strokes were shown by patients abbove age 45 and most of those patients are females."
      ],
      "metadata": {
        "id": "svU47VZ92c8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a violin plot as it offers an efficient means to visualize the age distribution within two distinct groups (individuals with and without a prior stroke). Additionally, it facilitates the comparison of gender distributions within each of these groups"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that individuals who have experienced a prevalent stroke tend to have a higher average age compared to those without a history of stroke. Furthermore, the chart reveals a greater presence of males in both groups, with a notably higher proportion of males observed among individuals with a prevalent stroke"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights obtained have the potential to assist healthcare institutions and insurance providers in formulating policy decisions concerning stroke prevention and treatment. For instance, these findings could contribute to informed choices regarding the allocation of specific preventive measures, like the use of blood thinners, or the structuring of stroke rehabilitation programs. Moreover, insurance firms may employ this data to shape their policies pertaining to stroke coverage and premium rates."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 13**\n",
        "**Is there any relation between individual with hypertensive and cigsperday?**"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# create a scatter plot of sysBP against cigsPerDay, colored by hypertension status\n",
        "sns.scatterplot(x='cigsPerDay', y='sysBP', hue='prevalentHyp', data=df)\n",
        "\n",
        "# add a title and axis labels\n",
        "plt.title('Relationship between Systolic Blood Pressure and Cigarettes Smoked per Day, by Hypertension Status')\n",
        "plt.xlabel('Cigarettes Smoked per Day')\n",
        "plt.ylabel('Systolic Blood Pressure')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a scatterplot as it is a suitable choice for visualizing the connection between two continuous variables, aligning with our specific interest in understanding the relationship between cigsPerDay and sysBP. Furthermore, the incorporation of color to signify hypertensive status provides a straightforward means of discerning potential data patterns or trends associated with hypertension status."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatterplot reveals a noticeable positive correlation between cigsPerDay and sysBP, irrespective of hypertension status. This implies that individuals who smoke a greater number of cigarettes per day tend to exhibit elevated systolic blood pressure levels. Furthermore, it is evident that individuals with prevalent hypertension generally demonstrate higher systolic blood pressure levels in comparison to those who do not have hypertension.\""
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gleaned from this chart hold promise for application in healthcare and wellness contexts, where the need to monitor blood pressure levels and mitigate cardiovascular risk factors, such as smoking, is paramount. By discerning the positive association between smoking and systolic blood pressure, healthcare providers have an opportunity to promote smoking cessation as a means to reduce blood pressure levels and mitigate the risk of hypertension and associated cardiovascular ailments.\n",
        "\n",
        "Notably, this chart does not reveal any indications of adverse trends. Nevertheless, should smoking cessation initiatives be implemented effectively and yield positive results, there could potentially be adverse repercussions for tobacco companies and the broader tobacco industry."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "sEGMxut24bCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,15))\n",
        "correlation = df.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"mako\", as_cmap=True))"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Correlation Heatmap because it provides an efficient means of visually representing the relationships between various pairs of features within a dataset. This visualization employs a color scheme to depict the strength of the correlation coefficient, facilitating the rapid identification of strongly correlated features"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Correlation Heatmap shows the pairwise correlation between all numerical features in the dataset.\n",
        "\n",
        "1) From corelation chart we can see that age is highly correlated with TenYearCHD by 22%. This suggests that these features may be important predictors of CHD risk.\n",
        "\n",
        "2) From the heatmap, we can see that age, systolic blood pressure, and diastolic blood pressure have a relatively strong correlation with the TenYearCHD target variable.  \n",
        "\n",
        "3) Additionally, we can see that there is a moderate positive correlation between systolic and diastolic blood pressure, by 78%.\n",
        "\n",
        "4) As well as diabetes and glucose are correlated by 61%.\n",
        "\n",
        "5) Also prevalent hypertension highly correlated with systolic blood pressure, and diastolic blood pressure by 70% and 61% respectively.\n",
        "\n",
        "6) And age is negatively correlated with education and cigarettes per day with 17% and 19% respectively."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Chart - 15 - Pair Plot**"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df[continous_variable])"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot serves as a valuable visualization tool for comprehending the interrelationships among the continuous variables present within the dataset. It facilitates the detection of both linear and non-linear correlations among these variables and aids in the recognition of potential outliers or unconventional patterns within the dataset"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the pair plot, it becomes evident that several variables exhibit positive correlations. Notably, age displays a positive association with systolic blood pressure, while BMI shows a similar correlation with glucose levels. Additionally, systolic blood pressure and diastolic blood pressure exhibit a linear relationship. A subtle positive correlation between cigsPerDay and sysBP is also discernible. Nevertheless, no distinct linear connection emerges between any of the variables and the target variable, TenYearCHD."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Patients with diabetes face an elevated risk of CHD compared to those without diabetes.\n",
        "2. Elevated total cholesterol levels are linked to a greater likelihood of coronary heart disease (CHD).\n",
        "\n",
        "3. The likelihood of being at risk for TenYearCHD is higher for individuals aged 50 and above."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "Patients with diabetes face an elevated risk of CHD compared to those without diabetes."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: There exists no substantial disparity in the likelihood of coronary heart disease (CHD) development between individuals with diabetes and those without the condition.\n",
        "\n",
        "**Alternative Hypothesis**: Individuals with diabetes exhibit an elevated risk of CHD development compared to those without diabetes."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on diabetic status\n",
        "diabetic = df[df['diabetes'] == 1]\n",
        "non_diabetic = df[df['diabetes'] == 0]\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean TenYearCHD rates of the two groups\n",
        "t_stat, p_val = stats.ttest_ind(diabetic['TenYearCHD'], non_diabetic['TenYearCHD'], equal_var=False)\n",
        "\n",
        "print('t_stat=%.3f, p_val=%.3f' % (t_stat, p_val))\n",
        "if p_val > 0.05:\n",
        "    print('Accept Null Hypothesis')\n",
        "else:\n",
        "    print('Reject Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-statistic serves as a metric for quantifying the disparity in means between patients with diabetes and those without, normalized by the standard error of this difference. Meanwhile, the p-value reflects the likelihood of encountering such a disparity in means purely due to random chance, assuming the null hypothesis holds.\n",
        "\n",
        "With a calculated p-value of 0.000, which is less than the predetermined significance level of 0.05, it strongly suggests that the probability of observing such a discrepancy in means by random chance alone is exceedingly low. Consequently, we reject the null hypothesis and infer that patients with diabetes face an elevated risk of developing coronary heart disease compared to their non-diabetic counterparts."
      ],
      "metadata": {
        "id": "eFt3GT4H8jTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was used to obtain the p-value for the hypothesis \"Patients with diabetes face an elevated risk of CHD compared to those without diabetes.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We opted for the two-sample t-test in this analysis because it is suitable for comparing the means of two distinct and independent groups, namely the diabetic and non-diabetic populations, in relation to the binary outcome variable of coronary heart disease (CHD) risk. This statistical test is well-suited for our purposes as it enables us to assess whether there exists a statistically significant disparity between the means of these two groups. Moreover, given the relatively substantial sample sizes of both groups, the t-test emerges as a resilient and dependable choice for conducting our analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hypothetical Statement - 2**\n",
        "Elevated total cholesterol levels are linked to a greater likelihood of coronary heart disease (CHD)."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)** - The mean total cholesterol levels do not differ significantly between the two groups.\n",
        "\n",
        "**Alternate Hypothesis (H1)** - There is a statistically significant difference in the mean total cholesterol levels between the two groups."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Import the required statistical test module from scipy\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on CHD status\n",
        "chd = df[df['TenYearCHD'] == 1] # Patients with CHD\n",
        "no_chd = df[df['TenYearCHD'] == 0] # Patients without CHD\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean total cholesterol levels of the two groups\n",
        "t_stat, p_val = stats.ttest_ind(chd['totChol'], no_chd['totChol'], equal_var=False)\n",
        "\n",
        "# Print the calculated t-statistic and p-value\n",
        "print('t_stat=%.3f, p_val=%.3f' % (t_stat, p_val))\n",
        "\n",
        "# Determine if the null hypothesis should be rejected based on the p-value\n",
        "if p_val < 0.05:\n",
        "    print('Reject the null hypothesis')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The observed p-value is highly significant (p_val=5.310852329016078e-07), well below the conventional significance threshold of 0.05.\n",
        "\n",
        "* Consequently, we have sufficient evidence to reject the null hypothesis, which posits no disparity in total cholesterol levels between the CHD and non-CHD groups.\n",
        "\n",
        "* These results imply an association between elevated total cholesterol levels and an increased susceptibility to coronary heart disease (CHD).\n",
        "\n",
        "* Additionally, the t-statistic of 5.065 reinforces this conclusion, signifying a noteworthy distinction in the means of the two groups"
      ],
      "metadata": {
        "id": "ZqvOMl1w66OH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conducted a two-sample t-test to calculate the p-value. This t-test was utilized to assess whether there exists a statistically significant distinction in the mean total cholesterol levels between two distinct groups: individuals with CHD and those without CHD."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the hypothesis that 'Elevated total cholesterol levels are linked to an increased risk of coronary heart disease (CHD),' the suitable statistical analysis to conduct would be a two-sample t-test. This choice is made because we are contrasting the mean total cholesterol levels between two distinct and independent groups: individuals with CHD and those without CHD. Given the dichotomous nature of the outcome variable (CHD status), it is imperative to assess whether a significant difference exists in total cholesterol levels between these two groups. The two-sample t-test is a widely employed statistical method for comparing the means of two independent groups. However, it is essential to note that this test assumes normal data distribution and equal variances in the two groups."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hypothetical Statement - 3**\n",
        "The likelihood of being at risk for TenYearCHD is higher for individuals aged 50 and above."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: \"There is no significant impact of age on the risk of TenYearCHD.\"\n",
        "\n",
        "**Alternative Hypothesis**: \"Individuals aged 50 and above exhibit a greater TenYearCHD risk compared to those below the age of 50.\""
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.stats.proportion as smp\n",
        "\n",
        "# Separate the dataset into two groups based on age\n",
        "above_50 = df[df['age'] > 50]\n",
        "below_50 = df[df['age'] <= 50]\n",
        "\n",
        "# Calculate the proportion of patients with TenYearCHD in each group\n",
        "prop_above_50 = above_50['TenYearCHD'].mean()\n",
        "prop_below_50 = below_50['TenYearCHD'].mean()\n",
        "\n",
        "# Perform a one-tailed z-test to compare the proportions of the two groups\n",
        "z_score, p_val = smp.proportions_ztest([prop_above_50 * len(above_50), prop_below_50 * len(below_50)], [len(above_50), len(below_50)], alternative='larger')\n",
        "\n",
        "print('z_score=%.3f, p_val=%.3f' % (z_score, p_val))\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print('Reject Null Hypothesis')\n",
        "else:\n",
        "    print('Accept Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the test indicate that the probability of observing a difference in the proportion of TenYearCHD risk between patients above 50 years of age and those below 50 years of age due to chance is very low.\n",
        "\n",
        "Rejected the null hypothesis and conclude that patients who are above 50 years of age are at a significantly higher risk of TenYearCHD than those who are below 50 years of age."
      ],
      "metadata": {
        "id": "Q9WlkMqa-TeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I conducted a one-tailed Z-test to assess the differences in proportions between patients with TenYearCHD above and below the age of 50."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a one-tailed z-test to assess the proportions between the two groups. The primary focus of our investigation is to determine whether the percentage of patients with TenYearCHD in the group aged above 50 years exceeds that of the group below 50 years. Utilizing a z-test is suitable in situations where we possess a substantial sample size and aim to compare proportions between two distinct groups"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no remaining null values in our dataset as we have already processed and handled them in data wrangling."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Handling Outliers & Outlier treatments\n",
        "fig, axes = plt.subplots(2, 4, figsize=(30, 15))\n",
        "axes = axes.flatten()\n",
        "for ax, col in zip(axes, continous_variable):\n",
        "    sns.boxplot(df[col], ax=ax)\n",
        "    ax.set_title(col.title(), weight='bold')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fuction to create dataframe of total outliers and percentage of outliers\n",
        "def outliers_df(df, continuous_features):\n",
        "    outlier_df = pd.DataFrame(columns=['feature', 'lower_limit', 'upper_limit',\n",
        "                                       'IQR', 'total_outliers', 'percentage_outliers(%)'])\n",
        "    for feature in continuous_features:\n",
        "        values = df[feature]\n",
        "        q1, q2, q3 = values.quantile([0.25, 0.5, 0.75])\n",
        "        iqr = q3 - q1\n",
        "        Lower_limit = q1 - 1.5 * iqr\n",
        "        Upper_limit = q3 + 1.5 * iqr\n",
        "        outliers = values[(values < Lower_limit) | (values > Upper_limit)]\n",
        "        total_outliers = len(outliers)\n",
        "        percentage_outliers = round(total_outliers * 100 / len(values), 2)\n",
        "        outlier_df = outlier_df.append({'feature': feature,\n",
        "                                        'lower_limit': Lower_limit,\n",
        "                                        'upper_limit': Upper_limit,\n",
        "                                        'IQR': iqr,\n",
        "                                        'total_outliers': total_outliers,\n",
        "                                        'percentage_outliers(%)': percentage_outliers},\n",
        "                                        ignore_index=True)\n",
        "    return outlier_df.sort_values(by=['percentage_outliers(%)'], ascending=False)"
      ],
      "metadata": {
        "id": "q0zYeVanB3QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_df(df,continous_variable)"
      ],
      "metadata": {
        "id": "lsbc5bJnB6-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Applying a blanket strategy of relocating all outliers into the 25-75 interquartile range may not be advisable for this dataset, especially when considering the possibility that some of these outliers might pertain to critically ill patients.\n",
        "\n",
        "* There exist several techniques for handling outliers within a dataset, including outlier removal, Winsorization, utilization of robust statistical methods, and data transformation.\n",
        "\n",
        "* Outlier removal involves the exclusion of data points identified as outliers. However, this approach comes with the drawback of potential information loss and a reduction in the sample size.\n",
        "\n",
        "* Therefore, in this context, we opt for data transformation, which entails applying mathematical functions such as logarithmic, square root, or reciprocal transformations. This approach can aid in normalizing the data distribution and mitigating the impact of outliers."
      ],
      "metadata": {
        "id": "HSSR9BDsCGDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying transformation for treating outlier\n",
        "df[continous_variable] = np.log(df[continous_variable] +1 )"
      ],
      "metadata": {
        "id": "FdaxPl8rCZpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I applied the LOG TRANSFORMATION technique to address outliers within the dataset.\n",
        "\n",
        "* I chose this approach due to its statistical nature and straightforward implementation, which has proven to yield effective results.\n",
        "\n",
        "* Additionally, this transformation can assist in normalizing the data distribution, rendering it more symmetrical"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "df['sex'] = pd.get_dummies(df['sex'], drop_first=True)\n",
        "df['is_smoking'] = pd.get_dummies(df['is_smoking'], drop_first=True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_vuAoMZdC0Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The rationale behind employing this technique lies in the fact that categorical variables are typically non-numeric in nature, whereas machine learning algorithms necessitate numerical input. Therefore, we have opted for one-hot encoding to convert the categorical variables 'sex' and 'is_smoking' into numerical equivalents, manifesting as binary values (0 or 1).\n",
        "\n",
        "* More precisely, we leverage the **get_dummies()** function provided by the pandas library to generate dummy variables, each serving as a distinct binary column representing the categories within each variable.\n",
        "\n",
        "* Furthermore, we utilize the **drop_first=True parameter** as a measure to mitigate potential multicollinearity issues within the dataset, which can arise when two dummy variables exhibit high correlation."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In correlation Heatmap we already seen that Systolic Blood Pressure and Diastolic Pressure are highly correlated.\n",
        "\n",
        "So we are Creating a new feature out of it to indicate whether an individual has a blood pressure issue or not."
      ],
      "metadata": {
        "id": "qsmivqJYEg6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GnVBX5EBEpOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After conducting a more in-depth examination of heart-related factors, it became evident that pulse pressure, defined as the disparity between systolic and diastolic blood pressure, exerts a significant influence on coronary heart disease (CHD). Consequently, we have the opportunity to construct a novel variable named 'PP' (pulse pressure) that amalgamates the systolic and diastolic blood pressure measurements into a unified column."
      ],
      "metadata": {
        "id": "94kjm6EtEzLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Adding pulse pressure as a column\n",
        "df['pulsePressure'] = df['sysBP'] - df['diaBP']"
      ],
      "metadata": {
        "id": "vkHKnbLpFAZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ze4_Y2HKFKaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Let's see how trip_duration and other features are related\n",
        "for col in df.describe().columns.tolist():\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df[col]\n",
        "    label = df['TenYearCHD']\n",
        "    correlation = feature.corr(label)\n",
        "    sns.scatterplot(x=feature, y=label, color=\"gray\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('TenYearCHD')\n",
        "    ax.set_title('TenYearCHD vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(df[col], df['TenYearCHD'], 1)\n",
        "    y_hat = np.poly1d(z)(df[col])\n",
        "    plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(abs(round(df.corr(),3)), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b690j2jjFY-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation analysis is a technique that entails assessing the degree of correlation between each feature and the target variable. Features exhibiting a strong correlation with the target variable are typically regarded as effective predictors and consequently included in the selection process"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \"Upon inspecting the heatmap, a conspicuous correlation emerges between sysBP and diaBP. Additionally, considering that we've already computed a new feature, namely pulsePressure, from these variables, we've decided to remove both 'sysBP' and 'diaBP' from the analysis.\"\n",
        "\n",
        "* \"We've opted to exclude the 'id' feature from our analysis as it doesn't carry significant importance for our analytical purposes.\"\n",
        "\n",
        "* \"Observing a substantial correlation between the 'is_smoking' and 'cigsPerDay' columns, we've chosen to eliminate one of them, particularly the one with a lesser influence on the target variable.\n",
        "\n",
        "* Furthermore, when the daily cigarette consumption exceeds zero, the 'smoking' column is assigned a value of 1, signifying a positive smoking status. Consequently, both statements convey equivalent information, leading us to drop the 'is_smoking' column.\""
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Creating Final DataFrame**"
      ],
      "metadata": {
        "id": "dwh0JDH2Gnav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Cu71l9_2GxHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = df[['age', 'education', 'sex','cigsPerDay', 'BPMeds',\n",
        "               'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol',\n",
        "               'BMI', 'heartRate', 'glucose', 'pulsePressure', 'TenYearCHD']]"
      ],
      "metadata": {
        "id": "1sqQPnbKGzgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for heatmap if anything remains to avoid multicollinearity\n",
        "plt.figure(figsize=(15,15))\n",
        "correlation = final_df.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"mako\", as_cmap=True))"
      ],
      "metadata": {
        "id": "HmycyvvnG6Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident that 'pulsePressure,' 'glucose,' and 'prevalentHyp' exhibit a moderate level of correlation with each other, suggesting their suitability for retention in the analysis."
      ],
      "metadata": {
        "id": "YXGPdo0iHEcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation is not required because we already did the transformation when treating outliers.\n",
        "\n",
        "But, we also updated our dataset, we added new feature as \"pulse pressure\".\n",
        "\n",
        "So, we will check for it, if it needs a transformation."
      ],
      "metadata": {
        "id": "4_rP4Hi9HiYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of pulse pressure\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"Before Applying Transformation\")\n",
        "sns.distplot(df['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "O8DhLVTaH1OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### If you want to check whether feature is guassian or normal distributed\n",
        "#### Q-Q plot\n",
        "stats.probplot(df['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "B8oT0x7nH9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 5 different copies to check the distribution of the variable\n",
        "test_df1=final_df.copy()\n",
        "test_df2=final_df.copy()\n",
        "test_df3=final_df.copy()\n",
        "test_df4=final_df.copy()"
      ],
      "metadata": {
        "id": "LCiwzEWBIFDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logarithmic Transformation**"
      ],
      "metadata": {
        "id": "9SN6rdcMIJrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "test_df1['pulsePressure']=np.log(test_df1['pulsePressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(df['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "Dp4oizYHIESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Q-Q plot\n",
        "stats.probplot(df['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "NS8NK5FhIDsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reciprocal Transformation**"
      ],
      "metadata": {
        "id": "qnjCXA0bIf4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "test_df2['pulsePressure']=1/(test_df2['pulsePressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(df['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "hfFILjWNIwtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Q-Q plot\n",
        "stats.probplot(df['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "xtKI3pf4I8nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Square Root Transformation**"
      ],
      "metadata": {
        "id": "VYXco4g-JCaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "test_df3['pulsePressure']=(test_df3['pulsePressure'])**(1/2)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(df['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "h3dp5ZKGJQEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Q-Q plot\n",
        "stats.probplot(df['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "xN5tN676JYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Transformation**"
      ],
      "metadata": {
        "id": "EMXuDiBqJdK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "test_df4['pulsePressure']=(test_df4['pulsePressure'])**(1/1.2)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(df['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "WJIB3xwVJiiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Q-Q plot\n",
        "stats.probplot(df['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "LwGMJ24FJoYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, however, it's important to note that we initially applied a logarithmic transformation to the data to address outliers. Subsequently, when dealing with the 'Pulse pressure' feature, we conducted further analysis and determined that it also benefitted from a logarithmic transformation to achieve optimal results.\""
      ],
      "metadata": {
        "id": "BitL1-lfJ3qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying transformation**"
      ],
      "metadata": {
        "id": "fW03EfFFKCXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Applying transformation on the considered column\n",
        "## Logarithmic transformation\n",
        "final_df['pulsePressure']=np.log(final_df['pulsePressure']+1)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "\n",
        "x= final_df.drop('TenYearCHD',axis=1)\n",
        "y= final_df[['TenYearCHD']]\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# Creating object\n",
        "std_regressor= StandardScaler()\n",
        "\n",
        "# Fit and Transform\n",
        "x= std_regressor.fit_transform(x)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The StandardScaler is a data scaling technique that transforms the data in such a way that its mean becomes 0, and its standard deviation becomes 1. This method is widely employed in machine learning for data scaling purposes due to its ability to maintain the original distribution's shape. It is particularly well-suited for most machine learning algorithms, especially those relying on distance-based metrics. Moreover, StandardScaler proves valuable when dealing with datasets featuring features that exhibit substantial differences in scale, as it aids in rendering these features more comparable.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "l332mT3-KrNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, it is not a required step in this context.\n",
        "\n",
        "In the context of the cardiovascular risk prediction dataset, there is no imperative need for dimensionality reduction. The dataset exhibits a relatively small number of features in comparison to the sample size, mitigating the risk of overfitting. Moreover, the dataset's size is modest, so the computational training time for machine learning models does not present a significant concern."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not needed"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's split data in the ratio of 80:20 where 80 % will be in training set and 20 % will be in testing set by using train_test_split function available in sklearn library"
      ],
      "metadata": {
        "id": "vzBiiKYOLdzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "Yw2ixgf1Lqbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of x_train is: {x_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of x_test is: {x_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "6VvpFPkWL4_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To mitigate the risk of overfitting and enhance the generalization capability of our model, we partitioned the data into a training set, comprising 80% of the data, and a testing set, which contained the remaining 20%. We employed the 'train_test_split' function available in the scikit-learn library for this purpose, as it is a widely adopted technique to facilitate model training and evaluation on distinct data subsets"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.TenYearCHD.value_counts())"
      ],
      "metadata": {
        "id": "79-ywHqhMfIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts of 'TenYearCHD' column\n",
        "counts =df['TenYearCHD'].value_counts()\n",
        "\n",
        "# set labels and colors for the pie chart\n",
        "labels = ['NO','YES']\n",
        "colors = ['skyblue','red']\n",
        "\n",
        "# create pie chart\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.pie(counts, labels=labels, colors=colors, autopct= \"%1.1f%%\",\n",
        "        startangle=90, shadow=True, explode=[0,0])\n",
        "\n",
        "\n",
        "#add title to the chart\n",
        "plt.title('TenYearCHD Distribution', fontsize=16)\n",
        "\n",
        "#display the chart\n",
        "plt.show"
      ],
      "metadata": {
        "id": "4OjWpiBCMxpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YES\n",
        "\n",
        "The pie chart clearly indicates that the target variable, which is the 10-year risk of coronary heart disease (CHD), is highly imbalanced. Out of the total sample population, 84.9% or 2879 individuals do not have the risk of CHD, while only 15.1% or 511 individuals are at risk. This significant class imbalance in the data could lead to biased predictions and can negatively impact the performance of machine learning models. Therefore, it is necessary to balance the data by applying appropriate techniques such as undersampling or oversampling to improve the accuracy and reliability of the models."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Fit and apply SMOTE to the data\n",
        "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
        "\n",
        "# Print the original and resampled dataset shapes\n",
        "print('Original dataset shape:', df.shape)\n",
        "print('Resampled dataset shape:', x_resampled.shape)\n",
        "\n",
        "# Count the number of samples in each class in the resampled dataset\n",
        "print('Class distribution in the resampled dataset:', y_resampled.value_counts())"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x_resampled, y_resampled,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "89R17H5bNXtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of x_train is: {x_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of x_test is: {x_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "e03KzgehNXQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I employed the Synthetic Minority Over-sampling Technique (SMOTE) to address the issue of imbalanced dataset. SMOTE is an oversampling method that creates synthetic data points for the minority class by interpolating new instances between the existing ones. This approach serves to balance the distribution of classes, mitigating the bias typically observed towards the majority class in imbalanced datasets. As a result, it can enhance the effectiveness of machine learning models when dealing with imbalanced datasets."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_metrics(y_train, y_test, train_preds, test_preds):\n",
        "    train_accuracy = accuracy_score(y_train, train_preds)\n",
        "    test_accuracy = accuracy_score(y_test, test_preds)\n",
        "    train_precision = precision_score(y_train, train_preds)\n",
        "    test_precision = precision_score(y_test, test_preds)\n",
        "    train_recall = recall_score(y_train, train_preds)\n",
        "    test_recall = recall_score(y_test, test_preds)\n",
        "    train_roc_auc = roc_auc_score(y_train, train_preds)\n",
        "    test_roc_auc = roc_auc_score(y_test, test_preds)\n",
        "\n",
        "    print(f\"{'Train Accuracy':<20}{train_accuracy:.4f}\")\n",
        "    print(f\"{'Test Accuracy':<20}{test_accuracy:.4f}\")\n",
        "    print(f\"{'Train Precision':<20}{train_precision:.4f}\")\n",
        "    print(f\"{'Test Precision':<20}{test_precision:.4f}\")\n",
        "    print(f\"{'Train Recall':<20}{train_recall:.4f}\")\n",
        "    print(f\"{'Test Recall':<20}{test_recall:.4f}\")\n",
        "    print(f\"{'Train ROC AUC':<20}{train_roc_auc:.4f}\")\n",
        "    print(f\"{'Test ROC AUC':<20}{test_roc_auc:.4f}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    train_confusion_matrix = confusion_matrix(y_train, train_preds)\n",
        "    test_confusion_matrix = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    labels = ['0', '1']\n",
        "    sns.heatmap(train_confusion_matrix, annot=True, cmap='Blues', ax=axes[0], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    axes[0].set_xlabel('Predicted labels')\n",
        "    axes[0].set_ylabel('True labels')\n",
        "    axes[0].set_title('Train Confusion Matrix')\n",
        "    sns.heatmap(test_confusion_matrix, annot=True, cmap='Blues', ax=axes[1], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    axes[1].set_xlabel('Predicted labels')\n",
        "    axes[1].set_ylabel('True labels')\n",
        "    axes[1].set_title('Test Confusion Matrix')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uhxlpqOwOfI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 1 Logistic Regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "logistic_classifier= LogisticRegression()\n",
        "# Fit the Algorithm\n",
        "logistic_classifier.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_logistic_pred= logistic_classifier.predict(x_train)\n",
        "y_test_logistic_pred= logistic_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_logistic_pred, y_test_logistic_pred)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import  metrics\n",
        "metrics.plot_roc_curve(logistic_classifier,x_test, y_test)"
      ],
      "metadata": {
        "id": "5EERk6Aca9lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "logistic_regression = LogisticRegression()\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'penalty': ['l1', 'l2'],\n",
        "              'C': [0.1, 1.0, 10.0],\n",
        "              'solver': ['liblinear', 'saga']}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# use the best hyperparameters to fit the model and make predictions\n",
        "logistic_regression_best = LogisticRegression(**best_params)\n",
        "# perform cross-validation on the model with the best hyperparameters\n",
        "cv_scores = cross_val_score(logistic_regression_best, x_train, y_train, cv=5)\n",
        "# fit the final model using all the training data and the best hyperparameters\n",
        "logistic_regression_best.fit(x_train, y_train)\n",
        "y_train_logistic_pred_cv = logistic_regression_best.predict(x_train)\n",
        "y_test_logistic_pred_cv  = logistic_regression_best.predict(x_test)\n",
        "y_score_logistic_pred_cv = logistic_regression_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV stands out as a potent method for fine-tuning the hyperparameters of machine learning models. This technique systematically assesses every conceivable combination of hyperparameters and their respective values. Subsequently, GridSearchCV identifies the optimal combination through performance evaluation, leading to enhanced model accuracy and improved overall performance"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We enhanced the performance of our machine learning model through the utilization of GridSearchCV, a technique employed to explore and identify the optimal hyperparameters. This approach exhaustively tested all possible combinations of hyperparameter values, ultimately selecting those that yielded the highest level of accuracy.\n",
        "\n",
        "Despite these efforts, we did not observe any significant enhancements in our results, with a **test accuracy of 67.97%**. **The test precision and recall stood at 66.55% and 69.27%, respectively**. Additionally, the area under the curve **(ROC AUC) score was 0.68**, which falls short of our desired benchmark.\n",
        "\n",
        "As a result, we plan to explore alternative models such as **Random Forest and XGBoost in pursuit of improved accuracy and a higher AUC score**"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model 2 - Random Forest Classifier**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2  Implementation\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_forest.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_rf_pred = random_forest.predict(x_train)\n",
        "y_test_rf_pred = random_forest.predict(x_test)"
      ],
      "metadata": {
        "id": "Nw5hK8pWeP4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_rf_pred, y_test_rf_pred)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "random_forest = RandomForestClassifier()\n",
        "param_grid = {'n_estimators': [100, 200, 300],\n",
        "              'max_depth': [5, 10, 15, None],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 2, 4]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# use the best hyperparameters to fit the model to the training data\n",
        "random_forest_best = RandomForestClassifier(**best_params)\n",
        "random_forest_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_rf_pred_gs = random_forest_best.predict(x_train)\n",
        "y_test_rf_pred_gs  = random_forest_best.predict(x_test)\n",
        "y_score_rf_pred_gs = random_forest_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_rf_pred_gs, y_test_rf_pred_gs)"
      ],
      "metadata": {
        "id": "NR3PWEUDe_PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use of GridSearchCV is a powerful method for fine-tuning the hyperparameters of machine learning models. By exhaustively searching through all possible combinations of hyperparameters and their values, GridSearchCV can identify the best combination for maximizing model performance, leading to more accurate results."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Through the use of GridSearchCV, we enhanced the performance of our machine learning model by identifying the optimal hyperparameters. GridSearchCV systematically evaluates all possible combinations of hyperparameters, enabling the selection of values that maximize model performance, ultimately leading to improved accuracy.\n",
        "\n",
        "* Following hyperparameter tuning, we determined the best parameters to be **'min_samples_leaf': 1, 'min_samples_split': 2, and 'n_estimators': 200.**\n",
        "\n",
        "* While hyperparameter tuning yielded a 100% train accuracy, it did not necessarily translate to the same level of performance on the test data. However, we were able to enhance the test accuracy significantly, raising it from **83.07% to 88.89%**.\n",
        "\n",
        "* Furthermore, our efforts also resulted in an **improved ROC AUC score, increasing from 0.8311 to 0.8890**"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The assessment of an ML model is pivotal in gauging the precision of its predictions. We utilized a range of metrics, encompassing Accuracy, Precision, Recall, and the ROC AUC score, to evaluate the alignment between the predicted values and the ground truth. The outcomes indicated that the model demonstrated an accuracy of roughly 88.89% in predicting Ten Year CHD. This degree of precision holds substantial importance, particularly considering that the target variable, TenYearCHD, bears direct relevance to business operations"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 3 - XGBoost Classifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "xgb = XGBClassifier()\n",
        "# Fit the Algorithm\n",
        "xgb.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_xgb_pred = xgb.predict(x_train)\n",
        "y_test_xgb_pred = xgb.predict(x_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_xgb_pred, y_test_xgb_pred)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'max_depth': [3, 5, 7],\n",
        "              'learning_rate': [0.01, 0.1, 0.3],\n",
        "              'n_estimators': [50, 100, 200]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# print the best hyperparameters\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "# Predict on the model\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_train_xgb_pred_gs = best_estimator.predict(x_train)\n",
        "y_test_xgb_pred_gs  = best_estimator.predict(x_test)\n",
        "y_score_xgb_pred_gs = best_estimator.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_xgb_pred_gs, y_test_xgb_pred_gs)"
      ],
      "metadata": {
        "id": "9jyKdtHejms2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to optimize the hyperparameters of our machine learning model, we employed the use of GridSearchCV. This method is highly effective as it evaluates all possible combinations of hyperparameters and their values, ultimately selecting the best combination based on performance calculations. This results in improved model performance and more accurate results."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized GridSearchCV to optimize the performance of our machine learning model by exhaustively evaluating all possible hyperparameter combinations to identify the optimal values. This led to more accurate results and improved model performance.\n",
        "\n",
        "We got best parameters as 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200 after hyperparameter tuning.\n",
        "\n",
        "The accuracy of our model improved significantly from 82.55% to 89.67%. We also saw improvements in the Precision and Recall metrics to 92.69% and 85.61%, respectively. Additionally, the ROC AUC score improved to 0.8958, which is considered good."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 4 - K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "qiytXWTrj9at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_knn_pred = knn.predict(x_train)\n",
        "y_test_knn_pred = knn.predict(x_test)"
      ],
      "metadata": {
        "id": "6PB-lM7akP3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Do9pQF7IkfvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_knn_pred, y_test_knn_pred)"
      ],
      "metadata": {
        "id": "iRwkcRCKkg_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "TKB-_yX2kmy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'n_neighbors': [3, 5, 7],\n",
        "              'weights': ['uniform', 'distance']}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "knn_best = KNeighborsClassifier(**best_params)\n",
        "knn_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_knn_pred_gs  = knn_best.predict(x_test)\n",
        "y_train_knn_pred_gs = knn_best.predict(x_train)\n",
        "y_score_knn_pred_gs = knn_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "h3sFKYujkrJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_knn_pred_gs, y_test_knn_pred_gs)"
      ],
      "metadata": {
        "id": "2BnXyJnWktVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NnrhJdFDkxZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the performance of our machine learning model, we utilized GridSearchCV to optimize the hyperparameters. This technique exhaustively evaluates all possible combinations of hyperparameters and their values, ultimately selecting the best combination for maximizing model performance. This approach leads to more accurate results and improved model performance."
      ],
      "metadata": {
        "id": "aL5ri8-Sk3VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "qU5ZW8luk3N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By leveraging GridSearchCV, we were able to optimize the performance of our machine learning model by exhaustively searching for the best hyperparameters through all possible combinations. As a result of selecting optimal values, our model's performance improved significantly.\n",
        "\n",
        "In the KNN model, we observed an improvement in accuracy from 78.56% to 82.12%, and a Precision of 74.02%, Recall of 97.69%, and ROC AUC score of 0.8246, which is higher after hyperparameter tuning, but lower than the previous model."
      ],
      "metadata": {
        "id": "7dm5h9OFlBl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5) Support Vector Machine Classifier (SVC)"
      ],
      "metadata": {
        "id": "QuxMqsCJlGjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "svc = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "\n",
        "# Fit the Algorithm\n",
        "svc.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_svc_pred = svc.predict(x_train)\n",
        "y_test_svc_pred = svc.predict(x_test)"
      ],
      "metadata": {
        "id": "xxe6nPNOlHgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "D1nDXLaElL3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_svc_pred, y_test_svc_pred)"
      ],
      "metadata": {
        "id": "wNULTJPBlJh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "1Gc6h3nLlQwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "svc = SVC(probability=True)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'kernel': ['linear', 'rbf'],\n",
        "              'gamma': ['scale', 'auto']}\n",
        "# perform a grid search with 5-fold cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "svc_best = SVC(**best_params, probability=True)\n",
        "svc_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_svc_pred_gs = svc_best.predict(x_test)\n",
        "y_train_svc_pred_gs = svc_best.predict(x_train)\n",
        "y_score_svc_pred_gs = svc_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "NaaU5LpSlRi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_svc_pred_gs, y_test_svc_pred_gs)"
      ],
      "metadata": {
        "id": "t_rFns4_lWWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "KdpudBKMlY2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using GridSearchCV to optimize the hyperparameters of our machine learning model, we were able to fine-tune the model for optimal performance. GridSearchCV evaluates all possible combinations of hyperparameters and their values to identify the best combination for maximizing model performance, leading to more accurate results and improved model performance."
      ],
      "metadata": {
        "id": "2wspq99blZs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "9nSXF4w4ldwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of our machine learning model was optimized by employing GridSearchCV to search for the best hyperparameters. GridSearchCV evaluates all possible combinations of hyperparameters and selects the optimal values to improve model performance and produce the most accurate results.\n",
        "\n",
        "After performing hyperparameter tuning, we observed a slight improvement in our model's performance. The accuracy increased from 70.14% to 76.74%, precision improved from 68.65% to 73.30%, and recall increased from 71.58% to 82.42%. We also achieved an AUC ROC of 76.86%."
      ],
      "metadata": {
        "id": "LU_t47Wrli02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 6- Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "s7INBEdplnmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation\n",
        "# create an instance of the Gaussian Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Fit the Algorithm\n",
        "nb.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_nb_pred = nb.predict(x_train)\n",
        "y_test_nb_pred = nb.predict(x_test)"
      ],
      "metadata": {
        "id": "dYQvFd18lsGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "QL_JWTESlwvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_nb_pred, y_test_nb_pred)"
      ],
      "metadata": {
        "id": "1B4rx-m1l0Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "bMAV2gKwl1GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# create an instance of the Gaussian Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "# perform a grid search with cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(nb, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# create a new instance of the classifier using the best hyperparameters\n",
        "nb_best = GaussianNB(**best_params)\n",
        "# evaluate the classifier using cross-validation\n",
        "scores = cross_val_score(nb_best, x_train, y_train, cv=5)\n",
        "# print the cross-validation scores\n",
        "print('Cross-validation scores:', scores)\n",
        "# train the classifier on the entire training set using the best hyperparameters\n",
        "nb_best.fit(x_train, y_train)\n",
        "# make predictions on the training and test sets\n",
        "y_train_nb_pred_gs = nb_best.predict(x_train)\n",
        "y_test_nb_pred_gs = nb_best.predict(x_test)\n",
        "y_score_nb_pred_gs = nb_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "8wTpDnHTl6rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_nb_pred_gs, y_test_nb_pred_gs)"
      ],
      "metadata": {
        "id": "P29M4i17l929"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "MY5Y-uJamAYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized GridSearchCV to fine-tune the hyperparameters of our machine learning model and improve its performance. GridSearchCV exhaustively searched through all possible combinations of hyperparameters to identify the best values for maximizing model performance."
      ],
      "metadata": {
        "id": "fy9kxZMrmESB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "V7NNUU3KmFGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance is almost similar, no significant improvement"
      ],
      "metadata": {
        "id": "puEspOUkmML2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"In the context of Predicting TenYearCHD, this is treated as a classification problem. In this scenario, the primary objective is to predict an outcome variable (TenYearCHD) based on one or more predictor variables.\n",
        "\n",
        "Evaluation metrics commonly employed for assessing the performance of a classification model include:\n",
        "\n",
        "**Accuracy**: This metric measures the proportion of correctly classified instances among all instances. A higher accuracy score signifies the model's superior ability to correctly predict the class for each instance.\n",
        "\n",
        "**Precision:** Precision quantifies the ratio of true positive predictions to all positive predictions made by the model. It is computed by dividing the number of true positives by the sum of true positives and false positives. A higher precision score indicates a lower rate of false positives, which holds significance in applications where the cost of false positives is substantial.\n",
        "\n",
        "**Recall:** Also known as sensitivity or true positive rate, recall measures the proportion of true positive predictions among all instances genuinely belonging to the positive class. It is determined by dividing the number of true positives by the sum of true positives and false negatives. A higher recall score signifies a lower rate of false negatives, which is particularly important in situations where false negatives carry a high cost.\n",
        "\n",
        "**AUC ROC (Area Under the Receiver Operating Characteristic Curve):** AUC ROC serves as a metric to assess the performance of binary classification models. It evaluates the model's capability to differentiate between positive and negative classes at various probability thresholds. The AUC ROC score ranges from 0 to 1, with 0.5 indicating a random model and 1 signifying a perfect model. A higher AUC ROC score indicates the model's superior ability to distinguish between positive and negative classes.\""
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the classifiers\n",
        "classifiers = [ (\"Logistic Regression\", LogisticRegression()),\n",
        "                (\"Random Forest Classifier\", RandomForestClassifier()),\n",
        "                (\"XGB Classifier\", XGBClassifier()),\n",
        "                (\"KNN\", KNeighborsClassifier()),\n",
        "                (\"SVC\", SVC(probability=True)),\n",
        "                (\"NB Classifier\", GaussianNB())]\n",
        "\n",
        "# iterate through classifiers and plot ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, classifier in classifiers:\n",
        "    classifier.fit(x_train, y_train)\n",
        "    y_score = classifier.predict_proba(x_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L4i-MLQJnRUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After cross validation and hyperparameter tuning**"
      ],
      "metadata": {
        "id": "REpl6BIbnVGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe\n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\",\"NBClassifier\"]\n",
        "Y_SCORE = [y_score_logistic_pred_cv, y_score_rf_pred_gs, y_score_xgb_pred_gs,\n",
        "           y_score_knn_pred_gs, y_score_svc_pred_gs,y_score_nb_pred_gs]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'MODEL': Model, 'Y_SCORE': Y_SCORE}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# plot the ROC curves for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, row in Metric_df.iterrows():\n",
        "    fpr, tpr, _ = roc_curve(y_test, row['Y_SCORE'])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{row['MODEL']} (AUC = {roc_auc:.2f})\", alpha=0.8)\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lasZECovnXc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe of metrics\n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model          = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\", \"NBClassifier\"]\n",
        "Test_Accuracy  = [0.6797,0.8889,0.8967,0.8212,0.7674,0.5885]\n",
        "Test_Precision = [0.6655,0.8796,0.9269,0.7402,0.7330,0.7330]\n",
        "Test_Recall    = [0.6927,0.8952,0.8561,0.9769,0.8242,0.2487]\n",
        "Test_ROC_AUC   = [0.6800,0.8890,0.8958,0.8246,0.7686,0.5810]\n",
        "# Create dataframe from the lists\n",
        "data = {'Model' : Model,\n",
        "        'Test_Accuracy'  : Test_Accuracy,\n",
        "        'Test_Precision' : Test_Precision,\n",
        "        'Test_Recall'    : Test_Recall,\n",
        "        'Test_ROC_AUC'   : Test_ROC_AUC}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "Metric_df"
      ],
      "metadata": {
        "id": "IqtAShgznb0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Considering the outcomes from various models evaluated within the cardiovascular risk prediction project, it becomes evident that the Random Forest Classifier and XGBoost models emerge as the most favorable choices for constructing the final prediction model. Both models exhibit impressive accuracy scores of 0.8889 and 0.8967, respectively, which are crucial attributes for real-time prediction systems. Furthermore, these models showcase commendable precision and recall scores, signifying their ability to accurately predict both positive and negative cases.\n",
        "\n",
        "* Although the KNN model demonstrates a relatively high recall score, its accuracy and precision metrics fall short of those achieved by the Random Forest Classifier and XGBoost models. Similarly, the SVC model displays lower accuracy and ROC AUC scores, implying its potential unsuitability for this specific classification task.\n",
        "\n",
        "* However, it's noteworthy that the XGBoost model outperforms the Random Forest Classifier marginally, boasting superior test accuracy and precision scores. This suggests that the XGBoost model may be the superior choice for forecasting cardiovascular risk.\n",
        "\n",
        "* Furthermore, the XGBoost model exhibits a higher ROC AUC score, indicating its enhanced ability to discriminate between positive and negative cases. Therefore, based on these findings, the XGBoost model stands out as the most fitting classification model for predicting cardiovascular risk in this project.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1yu9bedfnsQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although tree-based algorithms can be less interpretable, interpretability can be improved using tools like LIME and SHAP.\n",
        "\n",
        "Model interpretability can be approached globally and locally.\n",
        "\n",
        "1.  Global interpretability refers to understanding the overall relationship between features and prediction results. eg. Linear regression\n",
        "\n",
        "2. Local interpretability focuses on understanding the individual impact of each feature on a specific prediction. e.g. SHAP and LIME"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Global Explainability**"
      ],
      "metadata": {
        "id": "2IhzYdqkn9t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the barplot to determine which feature is contributing the most\n",
        "features = final_df.columns\n",
        "importances = best_estimator.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.grid(zorder=0)\n",
        "plt.title('Feature Importances', fontsize=20)\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "plt.yticks(range(len(indices)), features[indices])\n",
        "plt.xlabel('Relative Importance')"
      ],
      "metadata": {
        "id": "sbfXynNcoD1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ultimate model instance will be saved using the 'pickle' module for future reference. Pickling involves the conversion of a Python object into a byte stream, while unpickling is the reverse operation, which transforms a byte stream back into a Python object. This procedure is commonly referred to as serialization, marshalling, or flattening. The 'pickle' module facilitates the implementation of binary protocols to serialize and deserialize a Python object structure, rendering it valuable for purposes such as storing objects in a file, maintaining program state across sessions, or transmitting data across a network."
      ],
      "metadata": {
        "id": "nHQBoqkyoR_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pickle module\n",
        "import pickle\n",
        "# Save the File\n",
        "filename='Cardiovascular_Risk_Prediction_Classification.pkl'\n",
        "# serialize process (wb=write byte)\n",
        "pickle.dump(best_estimator,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# unserialize process (rb=read byte)\n",
        "Classification_model= pickle.load(open(filename,'rb'))\n",
        "\n",
        "# Predicting the unseen data(test set)\n",
        "Classification_model.predict(x_test)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we are getting the same predicted values\n",
        "y_test_xgb_pred_gs"
      ],
      "metadata": {
        "id": "6r-UNMOdok1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion From EDA**\n",
        "\n",
        "* Age emerges as a noteworthy factor influencing the risk of coronary heart disease (CHD).\n",
        "\n",
        "* The dataset suggests a higher predisposition for CHD among men compared to women.\n",
        "\n",
        "* Smoking emerges as a risk factor for CHD, with the intensity of smoking playing a role in determining this risk.\n",
        "\n",
        "* Patients with elevated blood pressure, a history of stroke, or diabetes exhibit a heightened risk for CHD.\n",
        "\n",
        "* Individuals with a history of stroke or hypertension are particularly susceptible to CHD.\n",
        "\n",
        "* Patients diagnosed with diabetes also demonstrate an elevated risk for CHD.\n",
        "\n",
        "* Total cholesterol levels show a modest elevation in patients at risk for CHD.\n",
        "\n",
        "* Noteworthy positive correlations are observed between specific variables, such as age and systolic blood pressure, as well as BMI and glucose levels.Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion From Model Implementation**\n",
        "\n",
        "1. Out of the six models that underwent testing, the Random Forest Classifier and XGBoost models demonstrated superior performance, boasting notably high accuracy, precision, and recall scores.\"\n",
        "\n",
        "2. \"While the KNN model exhibited a relatively commendable recall score, its accuracy and precision scores fell below those achieved by the Random Forest Classifier and XGBoost models.\"\n",
        "\n",
        "3. \"The SVC model, with its lower accuracy and ROC AUC score, appears less well-suited for this specific classification task.\"\n",
        "\n",
        "4. \"Comparatively, the XGBoost model outperformed the Random Forest Classifier, yielding slightly higher test accuracy and precision scores along with a superior ROC AUC score, implying its potential as a more effective choice for cardiovascular risk prediction.\"\n",
        "\n",
        "5. \"Based on the outcomes presented, the **XGBoost model** was selected as the optimal classification model for the cardiovascular risk prediction dataset, delivering an **impressive accuracy rate of 89.67%**\""
      ],
      "metadata": {
        "id": "PdpnyQODpAMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}